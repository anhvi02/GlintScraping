{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Driver setup and browser opened\n",
      "-- Glints accessed\n",
      "-- Process: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import smtplib\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# # Setting browser\n",
    "# Setting browser's options\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"--disable-popup-blocking\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "driver.implicitly_wait(0)\n",
    "print(\"-- Driver setup and browser opened\")\n",
    "\n",
    "# Access link\n",
    "sleep(1)\n",
    "driver.get('https://glints.com/vn/opportunities/jobs/explore')\n",
    "print('-- Glints accessed')\n",
    "\n",
    "# # Surpass website's noti\n",
    "# Scroll page to pop up the message\n",
    "sleep(7)\n",
    "html = driver.find_element(By.TAG_NAME,'html')\n",
    "html.send_keys(Keys.DOWN)\n",
    "\n",
    "# Escape\n",
    "sleep(5)\n",
    "html.send_keys(Keys.ESCAPE)\n",
    "\n",
    "# Extract data function\n",
    "# FUNCTION\n",
    "def extract(link):\n",
    "    element = WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div[class=\"GlintsContainer-sc-ap1z3q-0 iUnyrV\"]')))\n",
    "    html_of_interest = driver.execute_script('return arguments[0].innerHTML',element)\n",
    "    soup = BeautifulSoup(html_of_interest, 'lxml')\n",
    "\n",
    "    item_dict = {}\n",
    "\n",
    "    # Company Name\n",
    "    try:\n",
    "        item_dict['Name'] = soup.select('div[class=\"TopFoldsc__JobOverviewHeader-sc-kklg8i-22 ihxBLZ\"]')[0].text\n",
    "    except:\n",
    "        item_dict['Name'] = None \n",
    "\n",
    "    # Company Name\n",
    "    try:\n",
    "        item_dict['Company'] = soup.select('div[class=\"TopFoldsc__JobOverViewCompanyName-sc-kklg8i-5 eLQvRY\"]>a')[0].text\n",
    "    except:\n",
    "        item_dict['Company'] = None \n",
    "\n",
    "    # Location\n",
    "    try:\n",
    "        item_dict['Location'] = soup.select('div[class=\"TopFoldsc__JobOverViewCompanyLocation-sc-kklg8i-6 gLATOW\"]>span>a')[0].text\n",
    "    except:\n",
    "        item_dict['Location'] = None \n",
    "\n",
    "    info = soup.select('div[class=\"TopFoldsc__JobOverViewInfo-sc-kklg8i-9 EWOdY\"]')\n",
    "    info_list = []\n",
    "    for ite in info:\n",
    "        info_list.append(ite.text)\n",
    "    # Salary range \n",
    "    try:\n",
    "        salary_ele = soup.select('div[class=\"TopFoldsc__JobOverViewInfoContainer-sc-kklg8i-8 fgSCsF\"]>div>span')[0]\n",
    "        salary_txt = salary_ele.text \n",
    "        salary = salary_txt.split('D')[1].split('/')[0]\n",
    "        info_list.remove(salary_txt)\n",
    "        item_dict['Salary'] = salary\n",
    "    except:\n",
    "        item_dict['Salary']= None \n",
    "        \n",
    "    # Experienc, Type, Field`\n",
    "    item_dict['Experience'] = None\n",
    "    item_dict['Type'] = None \n",
    "\n",
    "    for ele in info_list:\n",
    "        if 'kinh nghiệm' in ele:\n",
    "            item_dict['Experience'] = ele \n",
    "            info_list.remove(ele)\n",
    "    for ele in info_list:  \n",
    "        if ('Việc' in ele) or ('Thực Tập' in ele):\n",
    "            item_dict['Type'] = ele\n",
    "            info_list.remove(ele)\n",
    "    try:\n",
    "        item_dict['Field'] = info_list[0]\n",
    "    except:\n",
    "        item_dict['Field'] = None\n",
    "\n",
    "    # Posted and Updated time\n",
    "    try:\n",
    "        posted = soup.select('span[class=\"TopFoldsc__PostedAt-sc-kklg8i-11 eRKLIR\"]')\n",
    "        posted = posted[0].text.split(' ',1)[1]\n",
    "        item_dict['Posted'] = posted\n",
    "    except:\n",
    "        item_dict['Posted'] = None\n",
    "\n",
    "    try:\n",
    "        updated = soup.select('span[class=\"TopFoldsc__UpdatedAt-sc-kklg8i-12 bYndtI\"]')\n",
    "        updated = updated[0].text.split(' ',1)[1]\n",
    "        item_dict['Updated'] = updated\n",
    "    except:\n",
    "        item_dict['Updated'] = None\n",
    "\n",
    "    #  Skill required\n",
    "    try:\n",
    "        skills = soup.select('div[class=\"TagStyle__TagContainer-sc-66xi2f-1 gtZZMG aries-tag Skillssc__TagOverride-sc-11imayw-3 fyJqX\"]')\n",
    "        skills_str = ''\n",
    "        for i in range(len(skills)):\n",
    "            if i == (len(skills) -1 ):\n",
    "                sub = skills[i].text\n",
    "            else:\n",
    "                sub = skills[i].text + ', '\n",
    "            skills_str = skills_str + sub\n",
    "        item_dict['Skill Required'] = skills_str\n",
    "    except:\n",
    "        item_dict['Skill Required'] = None \n",
    "\n",
    "    # Link\n",
    "    try:\n",
    "        item_dict['Link'] = link\n",
    "    except:\n",
    "        item_dict['Link'] = None \n",
    "    df_list.append(item_dict)\n",
    "\n",
    "\n",
    "# Import url from detail_urls\n",
    "#  import urls from the file\n",
    "urls = pd.read_csv('detail_urls.csv')\n",
    "urls = urls['Detail Urls'][709:]\n",
    "\n",
    "# Begin to scrap data\n",
    "print('-- Process:',end= ' ')\n",
    "sleep_time = 5\n",
    "try:\n",
    "    cnt = 0\n",
    "    df_list = []\n",
    "    total_urls = len(urls)\n",
    "\n",
    "    # Loop through detail_urls to get link\n",
    "    for link in urls.iloc:\n",
    "        driver.get(link)\n",
    "        sleep(sleep_time)\n",
    "        try: \n",
    "            extract(link)\n",
    "        except Exception as e:\n",
    "            pass \n",
    "            \n",
    "        cnt += 1\n",
    "        # PROGRESS ANNOUNCEMENT\n",
    "        if cnt%10 == 0:\n",
    "            print(f'{round((cnt/total_urls)*100,3)}%', end=' --> ')\n",
    "\n",
    "    # Notification\n",
    "    print(f'-- Successfully scraped all data!!')\n",
    "\n",
    "except Exception as err:\n",
    "    #  Notification\n",
    "    print(f'-- Error raised at url number {cnt}, program stopped')\n",
    "    print(e.__class__, \"occurred.\")\n",
    "    break\n",
    "\n",
    "\n",
    "# Save df to data file named ScrapedData\n",
    "df = pd.DataFrame(df_list)\n",
    "df.to_csv('ScrapedData2.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2f155fbeb9494e5ce992090b8427abe3542dae7719d8ea0d05cb0b78608edd18"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
